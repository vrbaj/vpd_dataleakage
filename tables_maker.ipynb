{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Table maker",
   "id": "e7fd283aaf14615e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T07:19:40.957260Z",
     "start_time": "2025-06-09T07:19:40.949711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "results_dir = Path(\".\", \"svd_results\")\n",
    "# results_stratified_dir = Path(\".\", \"results_minimal_stratified\")\n",
    "datasets = [\"svd\", \"voiced\"]\n",
    "scaling_transformations = set([str(file.name).split(\"_\")[1] for file in list(results_dir.glob(\"*.json\"))])\n",
    "# scaling_transformations = set([\"MinMaxScaler\", \"MaxAbsScaler\", \"QuantileTransformer\", \"RobustScaler\", \"StandardScaler\"])\n",
    "scaling_transformations = [\n",
    "    'MinMaxScaler',\n",
    "    'MaxAbsScaler',\n",
    "    'QuantileTransformer',\n",
    "    'RobustScaler',\n",
    "    'StandardScaler']"
   ],
   "id": "87a2c1c09f7f4ead",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Table with results for each scaler",
   "id": "b69d8d73c0c7518a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T07:20:24.490526Z",
     "start_time": "2025-06-09T07:19:41.023817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Iterate through datasets\n",
    "for dataset in datasets:\n",
    "    results_dir = Path(\".\", f\"{dataset}_results\")\n",
    "    results_stratified_dir = Path(\".\", f\"{dataset}_results_stratified\")\n",
    "    # Iterate through the scaler types\n",
    "    for scaler in scaling_transformations:\n",
    "        # Iterate through non-stratified and stratified results\n",
    "        for split_type, split_type_name in zip([results_dir, results_stratified_dir], [\"nonstratified\", \"stratified\"]):\n",
    "            scaler_results = pd.DataFrame(data=None, columns=None, index=[\"max\", \"min\", \"mean\", \"std\"])\n",
    "            # Iterate through individual model results for the given scaler type and split type\n",
    "            for result in split_type.glob(f\"*{scaler}*\"):\n",
    "                # Load the result\n",
    "                data = pd.read_json(result).transpose()\n",
    "                # Get the leakage and correct BCC values\n",
    "                data = pd.json_normalize(data[\"bcc\"])\n",
    "                # Calculate the difference\n",
    "                data[\"diff\"] = data[\"leakage\"] - data[\"correct\"]\n",
    "                # Get the metrics\n",
    "                col = data[[\"diff\"]].agg(func=[\"max\", \"min\", \"mean\", \"std\"])\n",
    "                # Name the column with the name of the classifier\n",
    "                col.columns = [result.stem.split(\"_\")[-1]]\n",
    "                # Concatenate the column with the performance of previous results\n",
    "                scaler_results = pd.concat([scaler_results, col], axis=1)\n",
    "            # Transpose the results so classifiers are rows\n",
    "            scaler_results = scaler_results.transpose().reset_index().rename(columns={\"index\": \"classifier\"})\n",
    "            # Once all results for the split type and scaler are added, save them\n",
    "            print(f\"scaler_{scaler}_split_{split_type_name}_dataset_{dataset}.csv\")\n",
    "            scaler_results.to_csv(f\"scaler_{scaler}_split_{split_type_name}_dataset_{dataset}.csv\", index=False)\n"
   ],
   "id": "3e4ef7e5e53bd240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler_MinMaxScaler_split_nonstratified_dataset_svd.csv\n",
      "scaler_MinMaxScaler_split_stratified_dataset_svd.csv\n",
      "scaler_MaxAbsScaler_split_nonstratified_dataset_svd.csv\n",
      "scaler_MaxAbsScaler_split_stratified_dataset_svd.csv\n",
      "scaler_QuantileTransformer_split_nonstratified_dataset_svd.csv\n",
      "scaler_QuantileTransformer_split_stratified_dataset_svd.csv\n",
      "scaler_RobustScaler_split_nonstratified_dataset_svd.csv\n",
      "scaler_RobustScaler_split_stratified_dataset_svd.csv\n",
      "scaler_StandardScaler_split_nonstratified_dataset_svd.csv\n",
      "scaler_StandardScaler_split_stratified_dataset_svd.csv\n",
      "scaler_MinMaxScaler_split_nonstratified_dataset_voiced.csv\n",
      "scaler_MinMaxScaler_split_stratified_dataset_voiced.csv\n",
      "scaler_MaxAbsScaler_split_nonstratified_dataset_voiced.csv\n",
      "scaler_MaxAbsScaler_split_stratified_dataset_voiced.csv\n",
      "scaler_QuantileTransformer_split_nonstratified_dataset_voiced.csv\n",
      "scaler_QuantileTransformer_split_stratified_dataset_voiced.csv\n",
      "scaler_RobustScaler_split_nonstratified_dataset_voiced.csv\n",
      "scaler_RobustScaler_split_stratified_dataset_voiced.csv\n",
      "scaler_StandardScaler_split_nonstratified_dataset_voiced.csv\n",
      "scaler_StandardScaler_split_stratified_dataset_voiced.csv\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T07:20:24.571074Z",
     "start_time": "2025-06-09T07:20:24.523591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code for table setup\n",
    "TABLE_START = \\\n",
    "\"\"\"\n",
    "\\\\begin{table}[t]\n",
    "\\t\\\\centering\n",
    "\\t\\\\begin{tabular}{llcccccccc}\n",
    "\\t\\t\\\\toprule\n",
    "\\t\\t & & \\\\multicolumn{4}{c}{Unstratified split} & \\\\multicolumn{4}{c}{Stratified split} \\\\\\\\\n",
    "\"\"\"\n",
    "\n",
    "# Encoding estimator names\n",
    "estimator_names = {\n",
    "    \"adaboost\":\"\\\\gls{AB}\",\n",
    "    \"dt\": \"\\\\gls{DT}\",\n",
    "    \"gaussianNB\": \"\\\\gls{GNB}\",\n",
    "    \"process\": \"\\\\gls{GP}\",\n",
    "    \"knn\": \"\\\\gls{KNN}\",\n",
    "    \"lda\": \"\\\\gls{LDA}\",\n",
    "    \"mlp\": \"\\\\gls{MLP}\",\n",
    "    \"qda\": \"\\\\gls{QDA}\",\n",
    "    \"rf\": \"\\\\gls{RF}\",\n",
    "    \"svm\": \"\\\\gls{SVM}\"\n",
    "    }\n",
    "\n",
    "# Encoding column names\n",
    "col_names = {\n",
    "    \"classifier\": \"Model\",\n",
    "    \"max\": \"Max\",\n",
    "    \"min\": \"Min\",\n",
    "    \"mean\": \"$\\\\mu$\",\n",
    "    \"std\": \"$\\\\sigma$\",\n",
    "}\n",
    "# Code for table end -> needs to be formatted to add the name of the tranformer\n",
    "table_end = \\\n",
    "\"\"\"\n",
    "\\t\\\\end{{tabular}}\n",
    "\\t\\\\caption{{Data leakage results for {transformer}}}\n",
    "\\t\\\\label{{tab:{transformer_label}}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "# Getting the name of the transformer\n",
    "table_latex = \"\"\n",
    "for transformer in scaling_transformations:\n",
    "    # Loading data\n",
    "    table_body = \"\"\n",
    "    for index, dataset in enumerate(datasets):\n",
    "        data = pd.read_csv(f\"scaler_{transformer}_split_nonstratified_dataset_{dataset}.csv\")\n",
    "        data_stratified = pd.read_csv(f\"scaler_{transformer}_split_stratified_dataset_{dataset}.csv\")\n",
    "        if index == 0:\n",
    "            # Creating a table header from the dataframe column names for the first dataset\n",
    "            table_body += \"\\t\\t & Model & \" + \" & \".join( [col_names[x] for x in data.columns.to_list()[1:]] * 2)  + \" \\\\\\\\\\n\"\n",
    "        else:\n",
    "            # For the second dataset, only line break is added\n",
    "            table_body += \" \\\\\\\\\\n\"\n",
    "        # Adding a midrule\n",
    "        table_body += \"\\t\\t\\\\midrule\\n\"\n",
    "        # Adding the dataset name -> because of formatted print, all curly brackets in the string have to be doubled\n",
    "        table_body += f\"\\t\\t\\\\multirow{{10}}{{*}}{{\\\\rotatebox[origin=c]{{90}}{{\\\\gls{{{dataset.upper()}}}}}}}\"\n",
    "        # Creating the table body with values for each model\n",
    "        for row1, row2 in zip(data.iterrows(), data_stratified.iterrows()):\n",
    "          # Saving only the values (discarding indices) as a list\n",
    "          row_as_list = row1[1].to_list() + row2[1].to_list()[1:]\n",
    "          # Formatting the values as floats with 3 decimals\n",
    "          row_formatted = [estimator_names[row_as_list[0]]] + [f\"{val:.3f}\" for val in row_as_list[1:]]\n",
    "          # Creating a latex code for the row with values\n",
    "          if row1[0] == 0:\n",
    "            table_row = \" & \" + \" & \".join(row_formatted)\n",
    "          else:\n",
    "            table_row = \" \\\\\\\\\\n\\t\\t & \" + \" & \".join(row_formatted)\n",
    "          # Adding the code to the existing table body\n",
    "          table_body += table_row\n",
    "        # Adding a bottomrule\n",
    "    table_body += \" \\\\\\\\\\n\\t\\t\\\\bottomrule\"\n",
    "\n",
    "    # Concatenating all strings of the final latex table\n",
    "    table_latex += TABLE_START + table_body + table_end.format(transformer=transformer, transformer_label=transformer.replace(\" \",\"_\").lower())\n",
    "\n",
    "# Saving the code as tex file\n",
    "with open(\"latex_tables.tex\", \"w\") as f:\n",
    "  f.write(table_latex)"
   ],
   "id": "4199dd3a98587422",
   "outputs": [],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
